# medgemmaWithRAG
Automatic Radiology Report Generation using RAG and LLMs
This project is an end-to-end pipeline for the automatic generation of CT scan reports using a combination of Vision-Language Models (VLMs) and Large Language Models (LLMs). The system leverages a RAG (Retrieval-Augmented Generation) architecture to enhance the accuracy and contextual relevance of the generated reports by referencing a local knowledge base.

Project Architecture
The project utilizes a two-stage process to convert an input image into a structured text report:

1. Image Analysis: A Vision-Language Model (VLM) analyzes the CT scan image and extracts preliminary findings as a text string.
2. RAG-Enhanced Report Generation: The preliminary findings, augmented with similar reports retrieved from a knowledge base, are fed into a Large Language Model (LLM) to generate the final, structured report.

[CT Image] -> [LLaVA Vision Model] -> [Preliminary Findings (Text)] -> [RAG Retriever (FAISS)] -> [Gemma LLM] -> [Final Report]
Features
End-to-End Pipeline: A complete workflow from image input to structured text report output.

Local Execution: All models run locally to ensure data privacy and security.

RAG-Based: Generates more accurate and consistent reports by leveraging your existing report archive.

Customizable: Easily swap out the LLM, VLM, and embedding models to suit your needs.

Technology Stack
Large Language Model (LLM): google/gemma-7b-it (or google/gemma-2b)

Vision-Language Model (VLM): llava-hf/llava-1.5-7b-hf

Embedding Model: pritamdeka/S-BioBert-snli-multinli-stsb

Vector Database: FAISS (Facebook AI Similarity Search)

Frameworks: LangChain, Transformers, PyTorch

‚ö° Setup and Installation
Follow these steps to get the project up and running.

1. Prerequisites
Python 3.10+

Git

An NVIDIA GPU with at least 16GB of VRAM (24GB recommended).

2. Clone the Repository
Bash

git clone <YOUR-REPOSITORY-URL>
cd <YOUR-PROJECT-DIRECTORY>
3. Directory Structure
Ensure you have the following directory structure in your project root:

/
|-- /data_reports           # Place all your .txt report files here
|-- /test_images            # Place sample CT scan images here
|-- /vector_store           # The vector database will be created here
|-- 1_prepare_knowledge_base.py
|-- 2_generate_report.py
`-- README.md
4. Install Dependencies
It is highly recommended to use a Python virtual environment.

Bash

# Create and activate a virtual environment
python -m venv venv
# On Windows:
venv\Scripts\activate
# On Linux/macOS:
source venv/bin/activate

# Install the required libraries from requirements.txt
pip install -r requirements.txt
Note: Your requirements.txt file should contain the following:

Plaintext

torch
torchvision
torchaudio
langchain
langchain-community
transformers
sentence-transformers
faiss-cpu # or faiss-gpu if on Linux
bitsandbytes
accelerate
pypdf
unstructured
Pillow
üöÄ How to Use
1. Prepare Data
Place all your anonymized, plain-text medical reports (as .txt files) into the data_reports folder.

2. Hugging Face Authentication
You need to authenticate with Hugging Face to download the Gemma models.

Bash

# Log in using the command line
huggingface-cli login
Then, visit the following model pages and accept their terms of service:

google/gemma-7b-it

llava-hf/llava-1.5-7b-hf

3. Build the Knowledge Base
Run the following script to process your reports and create the vector database. This only needs to be run once.

Bash

python 1_prepare_knowledge_base.py
4. Generate a Report
Place a sample CT scan image (e.g., sample.jpg) into the test_images folder.

Ensure the TEST_IMAGE_PATH variable in 2_generate_report.py points to your test image.

Run the main script to generate the report:

Bash

python 2_generate_report.py
After a few moments (depending on your hardware), the final report will be printed to the console.

‚öôÔ∏è Configuration
You can easily configure the models and key parameters at the top of the 2_generate_report.py file:

LLM_MODEL_ID: The identifier for the Large Language Model.

VLM_MODEL_ID: The identifier for the Vision-Language Model.

TEST_IMAGE_PATH: The path to the input image for testing.

‚ö†Ô∏è Important Disclaimer
This project is a research tool and a Proof of Concept (PoC). The output generated by this system should NEVER be used for actual clinical diagnosis or medical decision-making. All generated reports must be reviewed, edited, and verified by a qualified human radiologist.

üìÑ License
This project is licensed under the MIT License. See the LICENSE file for more details.
