import torch
from transformers import (
    AutoProcessor,
    LlavaForConditionalGeneration,
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline
)
from PIL import Image
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.llms import HuggingFacePipeline
import os
from datetime import datetime

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
VECTORSTORE_PATH = "vector_store"
EMBEDDING_MODEL_NAME = "pritamdeka/S-BioBert-snli-multinli-stsb"
VLM_MODEL_ID = "llava-hf/llava-1.5-7b-hf"          # Ù…Ø¯Ù„ Ø¨ÛŒÙ†Ø§ÛŒÛŒ (Vision)
LLM_MODEL_ID = "google/medgemma-4b-it"            # Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† (MedGemma 4B-IT)
# TEST_IMAGE_PATH = "test_images/images_medium_rg.2020200159.fig2.gif"  # ÙØ§ÛŒÙ„ GIF Ù‚Ø¨Ù„ÛŒ
TEST_IMAGE_PATH = "test_images/images_medium_rg.2020200159.fig2.gif"  # ÙØ§ÛŒÙ„ JPEG Ù…ÙˆØ¬ÙˆØ¯

# --- 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ---
def load_models():
    print("Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§...")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Ù…Ø¯Ù„ Ø¨ÛŒÙ†Ø§ÛŒÛŒ (LLaVA)
    vlm_processor = AutoProcessor.from_pretrained(VLM_MODEL_ID)
    vlm_model = LlavaForConditionalGeneration.from_pretrained(
        VLM_MODEL_ID,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    ).to(device)
    print("âœ… Ù…Ø¯Ù„ Ø¨ÛŒÙ†Ø§ÛŒÛŒ (LLaVA) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")

    # Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† (MedGemma 4B-IT)
    tokenizer = AutoTokenizer.from_pretrained(
        LLM_MODEL_ID,
        use_auth_token=True
    )
    llm_model = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL_ID,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        use_auth_token=True
    )
    pipe = pipeline(
        "text-generation",
        model=llm_model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        repetition_penalty=1.1
    )
    llm = HuggingFacePipeline(pipeline=pipe)
    print("âœ… Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† (MedGemma 4B-IT) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")

    # Ù…Ø¯Ù„ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': device}
    )
    print("âœ… Ù…Ø¯Ù„ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")

    return (vlm_processor, vlm_model, device), llm, embeddings

# --- 2. ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ---
def clean_report_output(text):
    """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ú¯Ø²Ø§Ø±Ø´ Ø§Ø² Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ùˆ ØªÚ©Ø±Ø§Ø±ÛŒ"""
    if not text:
        return ""
    
    # Ø­Ø°Ù Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…ÛŒ Ùˆ prompt Ù‡Ø§
    lines = text.split('\n')
    cleaned_lines = []
    
    skip_patterns = [
        'You are a professional radiologist',
        'IMAGE FINDINGS:',
        'CONTEXT FROM EXISTING REPORTS:',
        'FINAL REPORT',
        'Please provide',
        'USER:',
        'As a radiologist, describe',
        'ER:',
        '$\\boxed{',
        'Final Answer:',
        '---',
        'The final answer is'
    ]
    
    for line in lines:
        line = line.strip()
        if line and not any(pattern in line for pattern in skip_patterns):
            # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø®Ø·ÙˆØ·
            if not cleaned_lines or line != cleaned_lines[-1]:
                cleaned_lines.append(line)
    
    # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
    result = []
    for i, line in enumerate(cleaned_lines):
        if line or (i > 0 and cleaned_lines[i-1]):
            result.append(line)
    
    return '\n'.join(result).strip()

def get_findings_from_image(image_path, vlm_components):
    vlm_processor, vlm_model, device = vlm_components

    print(f"Ø¯Ø±Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ ØªØµÙˆÛŒØ±: {image_path}")
    try:
        raw_image = Image.open(image_path).convert('RGB')
    except FileNotFoundError:
        print(f"Ø®Ø·Ø§: ØªØµÙˆÛŒØ± Ø¯Ø± Ù…Ø³ÛŒØ± '{image_path}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return None

    vlm_prompt = (
        "USER: <image>\n"
        "As a radiologist, describe the key findings in this CT scan. "
        "Be factual and concise. Provide only the findings, not the instruction."
    )

    # Ø§ÛŒÙ†Ø¬Ø§: Ø§Ø² Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù†Ø§Ù…â€ŒØ¯Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ùˆ Ø³Ù¾Ø³ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ø¯Ø³ØªÚ¯Ø§Ù‡ Ù…ÛŒâ€ŒØ¨Ø±ÛŒÙ…
    inputs = vlm_processor(
        text=vlm_prompt,
        images=raw_image,
        return_tensors="pt"
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    output = vlm_model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=False
    )
    findings = vlm_processor.decode(
        output[0][2:],  # Ø­Ø°Ù ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø¢ØºØ§Ø²
        skip_special_tokens=True
    )

    print(f"ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ø² ØªØµÙˆÛŒØ±: {findings}")
    return findings

# --- 3. Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ ---
def main():
    vlm_components, llm, embeddings = load_models()

    print(f"Ø¯Ø±Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ø§Ø² Ù…Ø³ÛŒØ±: {VECTORSTORE_PATH}")
    vectorstore = FAISS.load_local(
        VECTORSTORE_PATH,
        embeddings,
        allow_dangerous_deserialization=True
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    # --- ØªÙ†Ø¸ÛŒÙ… prompt Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ---
    
    # 1. Prompt Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø¯ÙˆÙ† RAG
    template_english_no_rag = """
You are a professional radiologist. Write a structured CT scan report in English based ONLY on the image findings.

IMAGE FINDINGS:
{preliminary_findings}

Please provide a clean, structured report with FINDINGS and IMPRESSION sections:

FINDINGS:

IMPRESSION:
"""
    
    # 2. Prompt Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ø¯ÙˆÙ† RAG
    template_persian_no_rag = """
You are a professional radiologist. Write a structured CT scan report in Persian based ONLY on the image findings.

IMAGE FINDINGS:
{preliminary_findings}

Please provide a clean, structured report in Persian with ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ and Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ sections:

ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§:

Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:
"""
    
    # 3. Prompt Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø§ RAG
    template_english_rag = """
You are a professional radiologist. Write a structured CT scan report in English using the image findings and context from existing reports.

IMAGE FINDINGS:
{preliminary_findings}

CONTEXT FROM EXISTING REPORTS:
{context}

Please provide a clean, structured report with FINDINGS and IMPRESSION sections:

FINDINGS:

IMPRESSION:
"""
    
    # 4. Prompt Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ RAG
    template_persian_rag = """
You are a professional radiologist. Write a structured CT scan report in Persian using the image findings and context from existing reports.

IMAGE FINDINGS:
{preliminary_findings}

CONTEXT FROM EXISTING REPORTS:
{context}

Please provide a clean, structured report in Persian with ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ and Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ sections:

ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§:

Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:
"""

    # Ø§ÛŒØ¬Ø§Ø¯ prompt objects
    prompt_en_no_rag = PromptTemplate(template=template_english_no_rag, input_variables=["preliminary_findings"])
    prompt_fa_no_rag = PromptTemplate(template=template_persian_no_rag, input_variables=["preliminary_findings"])
    prompt_en_rag = PromptTemplate(template=template_english_rag, input_variables=["preliminary_findings", "context"])
    prompt_fa_rag = PromptTemplate(template=template_persian_rag, input_variables=["preliminary_findings", "context"])

    # Ø§ÛŒØ¬Ø§Ø¯ chain Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
    chain_en_no_rag = (
        {"preliminary_findings": RunnablePassthrough()} | prompt_en_no_rag | llm | StrOutputParser()
    )
    
    chain_fa_no_rag = (
        {"preliminary_findings": RunnablePassthrough()} | prompt_fa_no_rag | llm | StrOutputParser()
    )
    
    chain_en_rag = (
        {"context": retriever, "preliminary_findings": RunnablePassthrough()} | prompt_en_rag | llm | StrOutputParser()
    )
    
    chain_fa_rag = (
        {"context": retriever, "preliminary_findings": RunnablePassthrough()} | prompt_fa_rag | llm | StrOutputParser()
    )

    preliminary_findings = get_findings_from_image(TEST_IMAGE_PATH, vlm_components)
    if preliminary_findings:
        # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        preliminary_findings = clean_report_output(preliminary_findings)
        
        all_reports = ""
        
        # 1. Ú¯Ø²Ø§Ø±Ø´ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø¯ÙˆÙ† RAG
        print("\n" + "="*70)
        print("ğŸ” 1. English Report WITHOUT RAG")
        print("="*70)
        report_en_no_rag = chain_en_no_rag.invoke(preliminary_findings)
        report_en_no_rag = clean_report_output(report_en_no_rag)
        print(report_en_no_rag)
        print("------------------------------------------------------------------------------------")
        all_reports += "1. English Report WITHOUT RAG\n" + "="*70 + "\n" + report_en_no_rag + "\n" + "-"*84 + "\n\n"
        
        # 2. Ú¯Ø²Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ø¯ÙˆÙ† RAG
        print("\n" + "="*70)
        print("ğŸ” 2. Persian Report WITHOUT RAG")
        print("="*70)
        report_fa_no_rag = chain_fa_no_rag.invoke(preliminary_findings)
        report_fa_no_rag = clean_report_output(report_fa_no_rag)
        print(report_fa_no_rag)
        print("------------------------------------------------------------------------------------")
        all_reports += "2. Persian Report WITHOUT RAG\n" + "="*70 + "\n" + report_fa_no_rag + "\n" + "-"*84 + "\n\n"
        
        # 3. Ø§Ø¹Ù„Ø§Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² RAG
        print("\n" + "="*70)
        print("ğŸš€ Using RAG for generate better reports")
        print("="*70)
        
        # 4. Ú¯Ø²Ø§Ø±Ø´ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø§ RAG
        print("\n" + "="*70)
        print("ğŸ” 3. English Report WITH RAG")
        print("="*70)
        report_en_rag = chain_en_rag.invoke(preliminary_findings)
        report_en_rag = clean_report_output(report_en_rag)
        print(report_en_rag)
        print("------------------------------------------------------------------------------------")
        all_reports += "3. English Report WITH RAG\n" + "="*70 + "\n" + report_en_rag + "\n" + "-"*84 + "\n\n"
        
        # 5. Ú¯Ø²Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ RAG
        print("\n" + "="*70)
        print("ğŸ” 4. Persian Report WITH RAG")
        print("="*70)
        report_fa_rag = chain_fa_rag.invoke(preliminary_findings)
        report_fa_rag = clean_report_output(report_fa_rag)
        print(report_fa_rag)
        print("------------------------------------------------------------------------------------")
        all_reports += "4. Persian Report WITH RAG\n" + "="*70 + "\n" + report_fa_rag + "\n" + "-"*84 + "\n\n"
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªÙ…Ø§Ù… Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© ÙØ§ÛŒÙ„
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        filename = f"all_reports_{timestamp}.txt"
        
        final_reports_dir = "final-reports"
        os.makedirs(final_reports_dir, exist_ok=True)
        
        file_path = os.path.join(final_reports_dir, filename)
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write("CT SCAN REPORTS COMPARISON\n")
                f.write("Generated on: " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n")
                f.write("="*84 + "\n\n")
                f.write(all_reports)
            print(f"\nâœ… ØªÙ…Ø§Ù… Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {file_path}")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„: {e}")

if __name__ == "__main__":
    main()
